# [icml16-reinforcement](http://icml.cc/2016/?page_id=1649 "ICML 2016 Accepted Papers")



### Inverse Optimal Control with Deep Networks via Policy Optimization
Chelsea Finn, UC Berkeley; Sergey Levine, ; Pieter Abbeel, Berkeley

http://arxiv.org/abs/1603.00448

### Doubly Robust Off-policy Value Evaluation for Reinforcement Learning
Nan Jiang, University of Michigan; Lihong Li, Microsoft

http://arxiv.org/abs/1511.03722

### Smooth Imitation Learning
Hoang Le, Caltech; Andrew Kang, ; Yisong Yue, Caltech; Peter Carr,



### PAC Lower Bounds and Efficient Algorithms for The Max KK-Armed Bandit Problem
Yahel David, Technion; Nahum Shimkin, Technion



### Anytime Exploration for Multi-armed Bandits using Confidence Information
Kwang-Sung Jun, UW-Madison; Robert Nowak,



### Estimating Maximum Expected Value through Gaussian Approximation
Carlo D’Eramo, Politecnico di Milano; Marcello Restelli, Politecnico di Milano; Alessandro Nuara, Politecnico di Milano



### Markov Latent Feature Models
Aonan Zhang, Columbia University; John Paisley,



### The Knowledge Gradient for Sequential Decision Making with Stochastic Binary Feedbacks
Yingfei Wang, Princeton University; Chu Wang, ; Warren Powell,



### Learning to Filter with Predictive State Inference Machines
Wen Sun, Carnegie Mellon University; Arun Venkatraman, Carnegie Mellon University; Byron Boots, ; J.Andrew Bagnell, Carnegie Mellon University



### Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm
Junpei Komiyama, The University of Tokyo; Junya Honda, The University of Tokyo; Hiroshi Nakagawa, The University of Tokyo



### False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking
QianQian Xu, IIE, CAS; Jiechao Xiong, Peking University; Xiaochun Cao, Institute of information engineering, CAS; Yuan Yao, Peking University



### Benchmarking Deep Reinforcement Learning for Continuous Control
Yan Duan, University of California, Berk; Xi Chen, University of California, Berkeley; Rein Houthooft, Ghent University; John Schulman, University of California, Berkeley; Pieter Abbeel, Berkeley



### Ask Me Anything: Dynamic Memory Networks for Natural Language Processing
Ankit Kumar, MetaMind; Ozan Irsoy, MetaMind; Mohit Iyyer, MetaMind; James Bradbury, MetaMind; Ishaan Gulrajani, MetaMind; Victor Zhong, MetaMind; Romain Paulus, MetaMind; Richard Socher,



### Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control
Prashanth L.A., University of Maryland ; Cheng Jie, University of Maryland – College Park; Michael Fu, University of Maryland – College Park; Steve Marcus, University of Maryland – College Park; Csaba Szepesvari, Alberta



### ForecastICU: A Prognostic Decision Support System for Timely Prediction of Intensive Care Unit Admission
Jinsung Yoon, University of California, Los ; Ahmed Alaa, University of California, Los Angeles; Scott Hu, University of California, Los Angeles; Mihaela van der Schaar,



### An optimal algorithm for the Thresholding Bandit Problem
Andrea LOCATELLI, University of Potsdam; Maurilio Gutzeit, Universität Potsdam; Alexandra Carpentier,



### Sequential decision making under uncertainty: Are most decisions easy?
Ozgur Simsek, ; Simon Algorta, ; Amit Kothiyal,



### Gaussian quadrature for matrix inverse forms with applications
Chengtao Li, MIT; Suvrit Sra, ; Stefanie Jegelka, MIT



### Opponent Modeling in Deep Reinforcement Learning
He He, ; Jordan , ; Hal Daume, Maryland



### The knockoff filter for FDR control in group-sparse and multitask regression
Ran Dai, The University of Chicago; Rina Barber, The University of Chicago



### Softened Approximate Policy Iteration for Markov Games
Julien Pérolat, Univ. Lille; Bilal Piot, Univ. Lille; Matthieu Geist, ; Bruno Scherrer, ; Olivier Pietquin, Univ. Lille, CRIStAL, UMR 9189, SequeL Team, Villeneuve d’Ascq, 59650, FRANCE



### Asynchronous Methods for Deep Reinforcement Learning
Volodymyr Mnih, Google DeepMind; Adria Puigdomenech Badia, Google DeepMind; Mehdi Mirza, ; Alex Graves, Google DeepMind; Timothy Lillicrap, Google DeepMind; Tim Harley, Google DeepMind; David , ; Koray Kavukcuoglu, Google Deepmind



### Dueling Network Architectures for Deep Reinforcement Learning
Ziyu Wang, Google Inc.; Nando de Freitas, University of Oxford; Tom Schaul, Google Inc.; Matteo Hessel, Google Deepmind; Hado van Hasselt, Google DeepMind; Marc Lanctot, Google Deepmind



### Differentially Private Policy Evaluation
Borja Balle, Lancaster University; Maziar Gomrokchi, McGill University; Doina Precup, McGill



### Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning
Philip Thomas, CMU; Emma ,



### Hierarchical Decision Making In Electricity Grid Management
Gal Dalal, Technion; Elad Gilboa, Technion; Shie Mannor, Technion



### Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model
Xinze Guan, Oregon State University; Raviv Raich, Oregon State University; Weng-keen ,



### Generalization and Exploration via Randomized Value Functions
Ian Osband, Stanford; Ben , ; Zheng Wen, Adobe Research



### Dynamic Memory Networks for Visual and Textual Question Answering
Caiming Xiong, MetaMind; Stephen Merity, MetaMind; Richard Socher,



### Scalable Discrete Sampling as a Multi-Armed Bandit Problem
Yutian Chen, University of Cambridge; Zoubin ,



### Dynamic Capacity Networks
Amjad Almahairi, ; Nicolas Ballas, ; Tim Cooijmans, University of Montreal; Yin Zheng, Hulu LLC.; Hugo Larochelle, Twitter; Aaron Courville,



### Interacting Particle Markov Chain Monte Carlo
Tom Rainforth, University of Oxford; Christian Naesseth, Linköping University; Brooks Paige, University of Oxford; Frank Wood, ; Jan-Willem Vandemeent, ; Fredrik Lindsten, Uppsala University



### Meta–Gradient Boosted Decision Tree Model for Weight and Target Learning
Yury Ustinovskiy, Yandex; Valentina Fedorova, Yandex; Gleb Gusev, Yandex; Pavel Serdyukov, Yandex



### Model-Free Imitation Learning with Policy Optimization
Jonathan Ho, Stanford; Jayesh Gupta, Stanford University; Stefano Ermon,



### Memory-based Control of Active Perception and Action in Minecraft
Junhyuk Oh, University of Michigan; Valliappa Chockalingam, University of Michigan; Satinder , ; Honglak Lee, University of Michigan



### Improving the Efficiency of Deep Reinforcement Learning with Normalized Advantage Functions and Synthetic Experience
Shixiang Gu, University of Cambridge; Sergey Levine, Google; Timothy Lillicrap, Google DeepMind; Ilya Sutskever, OpenAI



### Epigraph projections for fast general convex programming
Po-Wei Wang, Carnegie Mellon University; Matt Wytock, ; Zico ,



### Near Optimal Behavior via Approximate State Abstraction
David Abel, Brown University; David Hershkowitz, Brown University; Michael Littman,



### Model-Free Trajectory Optimization for Reinforcement Learning of Motor Skills
Riad Akrour, TU Darmstadt; Gerhard Neumann,



