# [icml16-reinforcement](http://icml.cc/2016/?page_id=1649 "ICML 2016 Accepted Papers")



### 1 Inverse Optimal Control with Deep Networks via Policy Optimization
Chelsea Finn, UC Berkeley; Sergey Levine, ; Pieter Abbeel, Berkeley



### 2 Accurate Robust and Efficient Error Estimation for Decision Trees
Lixin Fan, Nokia Technologies



### 3 Structure Learning of Partitioned Markov Networks
Song Liu, The Inst. of Stats. Math.; Taiji Suzuki, ; Masashi Sugiyama, University of Tokyo; Kenji ,



### 4 Tracking Slowly Moving Clairvoyant: Optimal Dynamic Regret of Online Learning with True and Noisy Gradient
Tianbao Yang, University of Iowa; Lijun Zhang, Nanjing University; Rong Jin, ; Jinfeng Yi, IBM Research



### 5 Doubly Robust Off-policy Value Evaluation for Reinforcement Learning
Nan Jiang, University of Michigan; Lihong Li, Microsoft



### 6 Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing
Ke Li, UC Berkeley; Jitendra Malik,



### 7 PAC Lower Bounds and Efficient Algorithms for The Max KK-Armed Bandit Problem
Yahel David, Technion; Nahum Shimkin, Technion



### 8 Anytime Exploration for Multi-armed Bandits using Confidence Information
Kwang-Sung Jun, UW-Madison; Robert Nowak,



### 9 Estimating Maximum Expected Value through Gaussian Approximation
Carlo D’Eramo, Politecnico di Milano; Marcello Restelli, Politecnico di Milano; Alessandro Nuara, Politecnico di Milano



### 10 Markov Latent Feature Models
Aonan Zhang, Columbia University; John Paisley,



### 11 The Knowledge Gradient for Sequential Decision Making with Stochastic Binary Feedbacks
Yingfei Wang, Princeton University; Chu Wang, ; Warren Powell,



### 12 Learning to Filter with Predictive State Inference Machines
Wen Sun, Carnegie Mellon University; Arun Venkatraman, Carnegie Mellon University; Byron Boots, ; J.Andrew Bagnell, Carnegie Mellon University



### 13 Copeland Dueling Bandit Problem: Regret Lower Bound, Optimal Algorithm, and Computationally Efficient Algorithm
Junpei Komiyama, The University of Tokyo; Junya Honda, The University of Tokyo; Hiroshi Nakagawa, The University of Tokyo



### 14 False Discovery Rate Control and Statistical Quality Assessment of Annotators in Crowdsourced Ranking
QianQian Xu, IIE, CAS; Jiechao Xiong, Peking University; Xiaochun Cao, Institute of information engineering, CAS; Yuan Yao, Peking University



### 15 Benchmarking Deep Reinforcement Learning for Continuous Control
Yan Duan, University of California, Berk; Xi Chen, University of California, Berkeley; Rein Houthooft, Ghent University; John Schulman, University of California, Berkeley; Pieter Abbeel, Berkeley



### 16 Ask Me Anything: Dynamic Memory Networks for Natural Language Processing
Ankit Kumar, MetaMind; Ozan Irsoy, MetaMind; Mohit Iyyer, MetaMind; James Bradbury, MetaMind; Ishaan Gulrajani, MetaMind; Victor Zhong, MetaMind; Romain Paulus, MetaMind; Richard Socher,



### 17 Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control
Prashanth L.A., University of Maryland ; Cheng Jie, University of Maryland – College Park; Michael Fu, University of Maryland – College Park; Steve Marcus, University of Maryland – College Park; Csaba Szepesvari, Alberta



### 18 ForecastICU: A Prognostic Decision Support System for Timely Prediction of Intensive Care Unit Admission
Jinsung Yoon, University of California, Los ; Ahmed Alaa, University of California, Los Angeles; Scott Hu, University of California, Los Angeles; Mihaela van der Schaar,



### 19 An optimal algorithm for the Thresholding Bandit Problem
Andrea LOCATELLI, University of Potsdam; Maurilio Gutzeit, Universität Potsdam; Alexandra Carpentier,



### 20 Sequential decision making under uncertainty: Are most decisions easy?
Ozgur Simsek, ; Simon Algorta, ; Amit Kothiyal,



### 21 Gaussian quadrature for matrix inverse forms with applications
Chengtao Li, MIT; Suvrit Sra, ; Stefanie Jegelka, MIT



### 22 Opponent Modeling in Deep Reinforcement Learning
He He, ; Jordan , ; Hal Daume, Maryland



### 23 The knockoff filter for FDR control in group-sparse and multitask regression
Ran Dai, The University of Chicago; Rina Barber, The University of Chicago



### 24 Softened Approximate Policy Iteration for Markov Games
Julien Pérolat, Univ. Lille; Bilal Piot, Univ. Lille; Matthieu Geist, ; Bruno Scherrer, ; Olivier Pietquin, Univ. Lille, CRIStAL, UMR 9189, SequeL Team, Villeneuve d’Ascq, 59650, FRANCE



### 25 Asynchronous Methods for Deep Reinforcement Learning
Volodymyr Mnih, Google DeepMind; Adria Puigdomenech Badia, Google DeepMind; Mehdi Mirza, ; Alex Graves, Google DeepMind; Timothy Lillicrap, Google DeepMind; Tim Harley, Google DeepMind; David , ; Koray Kavukcuoglu, Google Deepmind



### 26 Dueling Network Architectures for Deep Reinforcement Learning
Ziyu Wang, Google Inc.; Nando de Freitas, University of Oxford; Tom Schaul, Google Inc.; Matteo Hessel, Google Deepmind; Hado van Hasselt, Google DeepMind; Marc Lanctot, Google Deepmind



### 27 Differentially Private Policy Evaluation
Borja Balle, Lancaster University; Maziar Gomrokchi, McGill University; Doina Precup, McGill



### 28 Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning
Philip Thomas, CMU; Emma ,



### 29 Hierarchical Decision Making In Electricity Grid Management
Gal Dalal, Technion; Elad Gilboa, Technion; Shie Mannor, Technion



### 30 Efficient Multi-Instance Learning for Activity Recognition from Time Series Data Using an Auto-Regressive Hidden Markov Model
Xinze Guan, Oregon State University; Raviv Raich, Oregon State University; Weng-keen ,



### 31 Generalization and Exploration via Randomized Value Functions
Ian Osband, Stanford; Ben , ; Zheng Wen, Adobe Research



### 32 Dynamic Memory Networks for Visual and Textual Question Answering
Caiming Xiong, MetaMind; Stephen Merity, MetaMind; Richard Socher,



### 33 Scalable Discrete Sampling as a Multi-Armed Bandit Problem
Yutian Chen, University of Cambridge; Zoubin ,



### 34 Dynamic Capacity Networks
Amjad Almahairi, ; Nicolas Ballas, ; Tim Cooijmans, University of Montreal; Yin Zheng, Hulu LLC.; Hugo Larochelle, Twitter; Aaron Courville,



### 35 Interacting Particle Markov Chain Monte Carlo
Tom Rainforth, University of Oxford; Christian Naesseth, Linköping University; Brooks Paige, University of Oxford; Frank Wood, ; Jan-Willem Vandemeent, ; Fredrik Lindsten, Uppsala University



### 36 Meta–Gradient Boosted Decision Tree Model for Weight and Target Learning
Yury Ustinovskiy, Yandex; Valentina Fedorova, Yandex; Gleb Gusev, Yandex; Pavel Serdyukov, Yandex



### 37 Model-Free Imitation Learning with Policy Optimization
Jonathan Ho, Stanford; Jayesh Gupta, Stanford University; Stefano Ermon,



### 38 Memory-based Control of Active Perception and Action in Minecraft
Junhyuk Oh, University of Michigan; Valliappa Chockalingam, University of Michigan; Satinder , ; Honglak Lee, University of Michigan



### 39 Improving the Efficiency of Deep Reinforcement Learning with Normalized Advantage Functions and Synthetic Experience
Shixiang Gu, University of Cambridge; Sergey Levine, Google; Timothy Lillicrap, Google DeepMind; Ilya Sutskever, OpenAI



### 40 Epigraph projections for fast general convex programming
Po-Wei Wang, Carnegie Mellon University; Matt Wytock, ; Zico ,



### 41 Near Optimal Behavior via Approximate State Abstraction
David Abel, Brown University; David Hershkowitz, Brown University; Michael Littman,



### 42 Model-Free Trajectory Optimization for Reinforcement Learning of Motor Skills
Riad Akrour, TU Darmstadt; Gerhard Neumann,



